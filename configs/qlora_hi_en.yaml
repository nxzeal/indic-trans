# v2: Using special control tokens (<FORMAL>, <INFORMAL>, <SIMPL_Y>, <SIMPL_N>)
# Training data: data/clean/hi_en/train_v2.tsv (migrate with scripts/migrate_to_control_tokens.py)
# Tokenizer must have special tokens added (run scripts/add_special_tokens_to_tokenizer.py first)

base_model: "models/indictrans2-indic-en-1B"
model_args:
  trust_remote_code: true
  use_fast_tokenizer: false
  allow_resize_token_embeddings: false
  local_files_only: true
pairs: ["hi-en"]
data_dir: "data/clean/hi_en"
output_dir: "outputs/hi_en_r8_v5"
seed: 42

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj","v_proj","k_proj","o_proj"]

quant: auto

quantization:
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

train:
  lr: 2.0e-4
  batch_size: 4
  grad_accum: 16
  max_steps: 3600
  warmup_ratio: 0.03
  do_eval: false
  eval_every: 300
  predict_with_generate: false
  per_device_eval_batch_size: 1
  save_every: 600
  gradient_checkpointing: true
  fp16: true
  save_total_limit: 2

task:
  max_src_len: 256
  max_tgt_len: 192
  tags:
    style: ["formal","informal"]
    simplify: ["yes","no"]
  prompt_template: "<SRC_LANG> <TGT_LANG> <STYLE> <SIMPLIFY> ||| <TEXT>"
  
