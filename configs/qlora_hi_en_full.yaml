# v2: Full training for strong control adherence (2 epochs)
# This is for Phase 2 after initial 3600-step validation
# Training data: data/clean/hi_en/train_v2.tsv

base_model: "models/indictrans2-indic-en-1B"
model_args:
  trust_remote_code: true
  use_fast_tokenizer: false
  allow_resize_token_embeddings: false
  local_files_only: true

pairs: ["hi-en"]
data_dir: "data/clean/hi_en"
output_dir: "outputs/hi_en_r8_v5_full"  # Different output dir
seed: 42

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj","v_proj","k_proj","o_proj"]

quant: auto

quantization:
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

train:
  lr: 2.0e-4
  batch_size: 4
  grad_accum: 16
  max_steps: 10000           # ~2 epochs for strong control learning
  warmup_ratio: 0.03
  do_eval: false
  eval_every: 500
  predict_with_generate: false
  per_device_eval_batch_size: 1
  save_every: 1000           # Save less frequently (disk space)
  gradient_checkpointing: true
  fp16: true
  save_total_limit: 3        # Keep 3 checkpoints

task:
  max_src_len: 256
  max_tgt_len: 192
  tags:
    style: ["formal","informal"]
    simplify: ["yes","no"]
  prompt_template: "<SRC_LANG> <TGT_LANG> <STYLE> <SIMPLIFY> ||| <TEXT>"
