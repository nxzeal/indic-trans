# v3: VOCABULARY TOKEN CONTROLS - Single-token approach
# Training with vocab-native control tokens (formal, casual, simple, detailed)
# This replaces the failed multi-token special token approach from v2
# Training data: data/clean/hi_en/train_v3.tsv

base_model: "models/indictrans2-indic-en-1B"
model_args:
  trust_remote_code: true
  use_fast_tokenizer: false
  allow_resize_token_embeddings: false
  local_files_only: true

pairs: ["hi-en"]
data_dir: "data/clean/hi_en"
output_dir: "outputs/hi_en_r8_v3"  # Fresh directory for v3 training
seed: 42

lora:
  r: 8
  alpha: 16
  dropout: 0.05
  target_modules: ["q_proj","v_proj","k_proj","o_proj"]

quant: auto

quantization:
  load_in_4bit: true
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: "nf4"
  bnb_4bit_compute_dtype: "float16"

train:
  lr: 2.0e-4
  batch_size: 4
  grad_accum: 16
  max_steps: 10000           # ~2 epochs for strong control learning
  warmup_ratio: 0.03
  do_eval: false
  eval_every: 500
  predict_with_generate: false
  per_device_eval_batch_size: 1
  save_every: 1000           # Checkpoints at 1k, 2k, 3k, etc.
  gradient_checkpointing: true
  fp16: true
  save_total_limit: 2        # Keep last 3 checkpoints

task:
  max_src_len: 256
  max_tgt_len: 192
  tags:
    style: ["formal","casual"]       # v3: vocab-native single tokens
    simplify: ["simple","detailed"]  # v3: vocab-native single tokens
  prompt_template: "<SRC_LANG> <TGT_LANG> <STYLE> <SIMPLIFY> ||| <TEXT>"
